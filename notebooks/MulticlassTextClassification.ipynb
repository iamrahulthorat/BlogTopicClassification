{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e341b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898b7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/blogtext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bd49d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dcc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f052339",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6aaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe with two columns\n",
    "df = df[[\"topic\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\"Technology\", \"Arts\", \"Education\", \"Communications-Media\", \"Internet\", \"Non-Profit\", \"Engineering\"]\n",
    "df = df.loc[df[\"topic\"].isin(options)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f25c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic_id\"] = df[\"topic\"].factorize()[0]\n",
    "\n",
    "topic_id_df = df[[\"topic\", \"topic_id\"]].drop_duplicates().sort_values(\"topic_id\")\n",
    "\n",
    "topic_to_id = dict(topic_id_df.values)\n",
    "id_to_topic = dict(topic_id_df[[\"topic_id\", \"topic\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56248b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00e81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('topic').text.count().plot.bar(ylim=0)\n",
    "plt.ylabel(\"Number of ocurrences\", fontsize = 10);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01efa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the \"text\" column\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_punctuation(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e373508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unwanted characters\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: re.sub('[^A-Za-z0-9]+', \" \", x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e231ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing whitespaces (begining and trailing spaces)\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: x.strip())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5263355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing more than 1 spaces with single space\n",
    "df[\"text\"] = df[\"text\"].str.replace(r\"\\s\\s+\", \" \")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "def tokenize(text):\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    return tokens\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: tokenize(x.lower()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords.append(\"urllink\")\n",
    "stopwords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e32d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: lemmatization(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecad3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Text processing using TF-IDF\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, max_features=5000, ngram_range=(1,2), stop_words=\"english\")\n",
    "\n",
    "# Transforming each text into a vector\n",
    "features = tfidf.fit_transform(df.text).toarray()\n",
    "labels = df.topic_id\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20dc1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the two most correlated terms with each of the sign categories\n",
    "N = 2\n",
    "for topic, topic_id in sorted(topic_to_id.items()):\n",
    "    features_chi2 = chi2(features, labels == topic_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names_out())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(\" \")) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(\" \")) == 2]\n",
    "    print(\"n-----> %s:\" %(topic))\n",
    "    print(\"  * Most Correlated Unigrams are: %s\" %(\", \".join(unigrams[-N:])))\n",
    "    print(\"  * Most Correlated Bigrams are: %s\" %(\", \".join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e299f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"] # Collection of blogs\n",
    "y = df[\"topic_id\"] # Target or the labels we want to predict (i.e. the different topics)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=42),\n",
    "]\n",
    "# Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring=\"accuracy\", cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=[\"model_name\", \"fold_idx\", \"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
